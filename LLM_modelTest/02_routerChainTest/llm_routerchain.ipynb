{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Router Chain Test\n",
    "#### *LLM 체인 라우팅 적용하기*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 00. 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('API_KEY_GEMINI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "lgdx_team2_routerchain\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"lgdx_team2_routerchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "# 랭체인 환경 설정\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "# VectorDB - FAISS\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 01. 벡터DB 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5196\\3142432257.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='ibm-granite/granite-embedding-278m-multilingual')\n",
      "c:\\workspaces\\LGDXteam2\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 임베딩 모델 생성\n",
    "# https://huggingface.co/ibm-granite/granite-embedding-278m-multilingual\n",
    "embeddings = HuggingFaceEmbeddings(model_name='ibm-granite/granite-embedding-278m-multilingual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어 로드\n",
    "new_vector_store = FAISS.load_local(\"../movie_4000_vectorstore_faiss\",\n",
    "                                    embeddings=embeddings,\n",
    "                                    allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 02. Router Chain: 사용자 질문 유형 구분\n",
    "\n",
    "- 정보검색\n",
    "- 추천요청\n",
    "- 일반대화 (`default_chain`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 사용자 입력값 유형 분류용 프롬프트\n",
    "classification_template = \"\"\"사용자의 입력을 다음 세 가지 범주 중 하나로 분류하세요:\n",
    "1️⃣ **\"정보검색\"**: 특정 영화, 드라마, 배우, 감독, 러닝타임, 개봉 연도, 수상 내역, 필모그래피 등 **사실적인 정보를 찾는 질문**\n",
    "   - 기대되는 응답 예시: 배우가 출연한 드라마/영화 목록, 특정 연도의 개봉작 리스트 등\n",
    "2️⃣ **\"추천요청\"**: 특정 장르, 배우, 테마(예: 좀비, 시간여행), 감성(예: 힐링, 긴장감) 등에 대한 **추천을 요청하는 질문**\n",
    "   - 기대되는 응답 예시: 특정 조건을 만족하는 영화/드라마 추천\n",
    "3️⃣ **\"일반대화\"**: 서비스와 무관한 일반적인 대화 (예: 날씨, AI 관련 질문, 잡담)\n",
    "\n",
    "#### **예시 형식**\n",
    "{{\n",
    "  \"type\": \"정보검색\"\n",
    "}}\n",
    "\n",
    "<user_input>\n",
    "{user_input}\n",
    "</user_input>\n",
    "\"\"\"\n",
    "classification_prompt = ChatPromptTemplate.from_template(classification_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLM을 이용한 질문 유형 분류 체인\n",
    "classification_chain = (\n",
    "  classification_prompt\n",
    "  | ChatGoogleGenerativeAI(model='gemini-1.5-flash', api_key=GEMINI_API_KEY)\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"type\": \"정보검색\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"추천요청\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"일반대화\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 질문 분류 테스트\n",
    "# 정보 검색 예상 질문\n",
    "print(classification_chain.invoke({'user_input': \"2023년에 개봉한 액션 영화 뭐 있어?\"}))\n",
    "\n",
    "# 추천 요청 예상 질문\n",
    "print(classification_chain.invoke({'user_input': '디카프리오가 주연한 영화 추천해줘.'}))\n",
    "\n",
    "# 일반 대화 예상 질문\n",
    "print(classification_chain.invoke({'user_input': \"너가 제일 좋아하는 영화 뭐야?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 결과를 올바르게 파싱하는 함수\n",
    "def preprocess_classification_result(user_input: str):\n",
    "    classification_result = classification_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "    # JSON 문자열에서 ```json``` 제거 (정규식 활용)\n",
    "    json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', classification_result, re.DOTALL)\n",
    "    if json_match:\n",
    "        clean_json_str = json_match.group(1)                       # 중괄호 {} 내부만 추출\n",
    "    else:\n",
    "        clean_json_str = classification_result                     # ```json``` 태그가 없을 경우 그대로 사용\n",
    "\n",
    "    # JSON 문자열을 dict 자료형으로 변환\n",
    "    try:\n",
    "        classification_data = json.loads(clean_json_str)\n",
    "        type_value = classification_data.get(\"type\", \"일반대화\")   # 기본값을 '일반대화'로 설정\n",
    "        # keywords = classification_data.get(\"keywords\", [])         # 기본값을 빈 리스트로 설정\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"⚠️ JSONDecodeError: {e}\")  # JSON 변환 오류 확인\n",
    "        type_value = \"일반대화\"           # JSON 변환 오류 시 기본값 설정\n",
    "        # keywords = []\n",
    "    \n",
    "    # return {\"type\": type_value, \"keywords\": keywords}\n",
    "    return {\"type\": type_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리 자료형을 string으로 변환하는 함수\n",
    "def format_change(classification_result: dict) -> str:\n",
    "    type_value = classification_result.get(\"classification_result\", {}).get(\"type\", \"일반대화\")\n",
    "    # keywords = classification_result.get(\"classification_result\", {}).get(\"keywords\", [])\n",
    "    user_input = classification_result.get(\"user_input\", \"\")\n",
    "\n",
    "    # string 자료형으로 변경\n",
    "    # formatted_str = f\"type: '{type_value}', keywords: {keywords}, user_input: '{user_input}'\"\n",
    "    formatted_str = f\"type: '{type_value}', user_input: '{user_input}'\"\n",
    "    return formatted_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 03. Destination Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1. default-chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> model loaded...\n"
     ]
    }
   ],
   "source": [
    "# default_chain 생성 (사용자의 의미없는 입력값에 대해 정해진 답변을 할 때)\n",
    "# 프롬프트 템플릿 설정\n",
    "default_template = \"\"\"\n",
    "\"You are a chatbot that must always respond with '🐶: 멍멍!'.\n",
    "No matter what question the user asks, always reply with '🐶: 멍멍!'\"\n",
    "\n",
    "[사용자 입력과 분류 결과]:\n",
    "{classification_result}\n",
    "\"\"\"\n",
    "default_prompt = ChatPromptTemplate.from_template(default_template)\n",
    "\n",
    "# Google Gemini 모델 생성\n",
    "def load_gemini():\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model='gemini-1.5-flash',\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        api_key=GEMINI_API_KEY\n",
    "    )\n",
    "    print(\">>>>>>> model loaded...\")\n",
    "    return model\n",
    "\n",
    "default_llm = load_gemini()\n",
    "\n",
    "# langchain 체인 구성\n",
    "default_chain = (\n",
    "  {\"classification_result\": RunnablePassthrough()}\n",
    "  | default_prompt               # 하나로 만든 문서를 prompt에 넘겨주고\n",
    "  | default_llm            # llm이 원하는 답변을 만듦\n",
    "  # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2. search-chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "#### *3. recommendation-chain*\n",
    "- 추천 목록을 반환하는 체인\n",
    "- `next_input`이 필요 없음 + 정해진 자료형으로 답해야 함  \n",
    "  => `RouterOutputParser`를 사용해보는 게 좋을 거 같음..~\n",
    "- 이 체인 뒤에 사용자 시청 기록 기반으로 반환된 추천 목록에서 5개를 정하는 작업을 해야 함\n",
    "\n",
    "<br>\n",
    "\n",
    "- 장르 기반 추천 ⇒ 장르\n",
    "- 줄거리(키워드/컨셉) 기반 추천 ⇒ 줄거리\n",
    "- 특정 연도 및 시대별 콘텐츠 추천 ⇒ 줄거리에 언급되는 시대배경/연도\n",
    "==================================================\n",
    "- 콘텐츠 정보 기반 추천 ⇒ 어떤 행이 있는지 봐야할듯\n",
    "- 인기 있는 콘텐츠 추천 ⇒ 시청횟수가 많은 것\n",
    "- 리뷰(감성) 기반 추천 ⇒ 리뷰\n",
    "- 사용자 선호 기반 추천 ⇒ 사용자 시청기록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> model loaded...\n"
     ]
    }
   ],
   "source": [
    "# 검색기 생성\n",
    "retriever = new_vector_store.as_retriever(\n",
    "    search_type=\"mmr\",   \n",
    "    search_kwargs={\"k\": 20,              # 반환할 문서 수 (default: 4)\n",
    "                   \"fetch_k\": 50,       # MMR 알고리즘에 전달할 문서 수\n",
    "                   \"lambda_mult\": 0.5,    # 결과 다양성 조절 (default: 0.5),\n",
    "                   }\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "template = \"\"\"\n",
    "You are a movie-recommendation chatbot.\n",
    "You must only answer based on the given context.\n",
    "Do not generate answers that are not directly supported by the context.\n",
    "아래 JSON 리스트 형식으로 반환하세요.  \n",
    "JSON 이외의 설명은 하지 마세요.  \n",
    "추천 순서는 `id`로 표시하며, `genre`(장르)와 `title`(영화 제목)은 retrieved_context에서 가져온 메타데이터를 기반으로 합니다.\n",
    "\n",
    "---\n",
    "예제 출력 형식:\n",
    "```json\n",
    "{{\n",
    "    {{\"id\": 1, \"genre\": \"다큐멘터리\", \"title\": \"견자단의 용호무\"}},\n",
    "    {{\"id\": 2, \"genre\": \"드라마\", \"title\": \"스프링 송\"}},\n",
    "    {{\"id\": 3, \"genre\": \"드라마\", \"title\": \"디어 마이 프렌드\"}}\n",
    "}}\n",
    "\n",
    "[사용자 입력과 사용자 입력값의 유형 및 키워드]:\n",
    "{user_input}\n",
    "\n",
    "[Context]:\n",
    "{retrieved_context}\n",
    "\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Google Gemini 모델 생성\n",
    "def load_gemini(system_instruction):\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model='gemini-1.5-flash',\n",
    "        temperature=0.3,\n",
    "        max_tokens=5000,\n",
    "        system_instruction=system_instruction,\n",
    "        api_key=GEMINI_API_KEY\n",
    "    )\n",
    "    print(\">>>>>>> model loaded...\")\n",
    "    return model\n",
    "\n",
    "system_instruction = \"\"\"you are a movie-recommendation chatbot. you must answer based on given data.\"\"\"\n",
    "llm = load_gemini(system_instruction)\n",
    "\n",
    "# langchain 체인 구성\n",
    "recommendation_chain = (\n",
    "  {\"user_input\":RunnablePassthrough(),\n",
    "    \"retrieved_context\": retriever,\n",
    "  }\n",
    "  # question(사용자의 질문) 기반으로 연관성이 높은 문서 retriever 수행 >> format_docs로 문서를 하나로 만듦\n",
    "  | prompt               # 하나로 만든 문서를 prompt에 넘겨주고\n",
    "  | llm                  # llm이 원하는 답변을 만듦\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 04. Full Chain 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_input(classification_result: dict):\n",
    "    # 사용자의 입력 유형 분류\n",
    "    classification_data = classification_result.get(\"classification_result\", {})  # 내부 딕셔너리 추출\n",
    "    type_value = classification_data.get(\"type\", \"일반대화\")  # 기본값 설정\n",
    "    # keywords = classification_data.get(\"keywords\", [])  # 기본값 설정\n",
    "    \n",
    "    print(f\"===================== Type: {type_value}\")\n",
    "    # print(f\"===================== Keywords: {keywords}\")\n",
    "    \n",
    "    if type_value == '정보검색':\n",
    "        return \"정보검색 체인 실행은 여기!!!\"\n",
    "    elif type_value == '추천요청':\n",
    "        formatted_string = format_change(classification_result)\n",
    "        return recommendation_chain.invoke(formatted_string)\n",
    "    else:\n",
    "        return default_chain.invoke({\"classification_result\": classification_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "  {\"classification_result\": RunnableLambda(preprocess_classification_result),\n",
    "   \"user_input\":itemgetter(\"user_input\")}\n",
    "  | RunnableLambda(process_user_input)\n",
    "  | StrOutputParser()  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Type: 추천요청\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n  {\\n    \"id\": \"f09b0b34-df98-48df-a722-7837b5e71f33\",\\n    \"genre\": \"액션/어드벤쳐\",\\n    \"title\": \"액션히어로\"\\n  }\\n]\\n```\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"user_input\": \"미국 뉴욕을 배경으로 한 액션 영화를 추천해줘.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
